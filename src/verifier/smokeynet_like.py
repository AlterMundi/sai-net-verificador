"""
SmokeyNet-like Architecture - Sacred Implementation
Following exact specifications from the divine documentation.

Architecture components (as per sacred roadmap):
1. CNN Backbone: ResNet-34 pretrained (ImageNet) per tile
2. Temporal Aggregation: LSTM bidirectional (2 layers, hidden=512) 
3. Spatial/Global Aggregation: ViT-S (6-8 blocks, dim=768, heads=12)
4. Heads: Global (binary smoke/no-smoke) + Auxiliary tile heads (optional)

Sacred parameters:
- tiles=45 de 224Ã—224px
- L=3 temporal window
- PÃ©rdida: L = Î»_global * BCE + Î»_tiles * BCE_tiles (1.0 y 0.3)
- Target: F1â‰ˆ82.6%, TTDâ‰¤4 min, Recallâ‰¥80%
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import math
from typing import Tuple, Optional, Dict, Any


class TileEncoder(nn.Module):
    """
    CNN backbone for tile feature extraction.
    Sacred spec: ResNet-34 pretrained (ImageNet).
    """
    
    def __init__(self):
        super().__init__()
        # ResNet-34 pretrained as specified in sacred documentation
        self.backbone = models.resnet34(weights='IMAGENET1K_V1')
        
        # Remove final classification layer (replace with Identity)
        self.backbone.fc = nn.Identity()
        
        # Output dimension is 512 for ResNet-34
        self.feature_dim = 512
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for tile encoding.
        
        Args:
            x: Input tensor [batch_size, channels, height, width]
            Expected: [B, 3, 224, 224] per tile
            
        Returns:
            features: [batch_size, 512]
        """
        return self.backbone(x)


class TemporalAggregator(nn.Module):
    """
    LSTM for temporal modeling across frames.
    Sacred spec: bidirectional (2 layers, hidden=512).
    """
    
    def __init__(self, input_size: int = 512, hidden_size: int = 512, num_layers: int = 2):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,  # Sacred specification
            dropout=0.1
        )
        
        # Bidirectional doubles the output size
        self.output_dim = hidden_size * 2
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for temporal aggregation.
        
        Args:
            x: Input tensor [batch_size, sequence_length, feature_dim]
            Expected: [B, L, 512] where L=3 (sacred window)
            
        Returns:
            output: [batch_size, sequence_length, hidden_size*2]
        """
        output, (hidden, cell) = self.lstm(x)
        return output


class ViTEncoder(nn.Module):
    """
    Vision Transformer for spatial-global reasoning.
    Sacred spec: ViT-S (6-8 blocks, dim=768, heads=12).
    """
    
    def __init__(self, dim: int = 768, depth: int = 6, heads: int = 12, mlp_dim: int = 3072):
        super().__init__()
        
        self.dim = dim
        self.depth = depth
        self.heads = heads
        
        # Positional embeddings for tiles (45 tiles + 1 CLS token)
        self.num_tiles = 45  # Sacred specification
        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tiles + 1, dim))
        
        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
        # Transformer blocks
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(dim, heads, mlp_dim)
            for _ in range(depth)
        ])
        
        # Layer normalization
        self.norm = nn.LayerNorm(dim)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass for ViT encoding.
        
        Args:
            x: Input tensor [batch_size, num_tiles, feature_dim]
            Expected: [B, 45, feature_dim] for 45 tiles
            
        Returns:
            cls_output: [batch_size, dim] - Global representation
            tile_outputs: [batch_size, num_tiles, dim] - Per-tile representations
        """
        batch_size = x.shape[0]
        
        # Add CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # [B, num_tiles+1, dim]
        
        # Add positional embeddings
        x += self.pos_embedding
        
        # Apply transformer blocks
        for block in self.transformer_blocks:
            x = block(x)
        
        # Final normalization
        x = self.norm(x)
        
        # Separate CLS and tile outputs
        cls_output = x[:, 0]  # [B, dim]
        tile_outputs = x[:, 1:]  # [B, num_tiles, dim]
        
        return cls_output, tile_outputs


class TransformerBlock(nn.Module):
    """Single Transformer block with multi-head attention and MLP."""
    
    def __init__(self, dim: int, heads: int, mlp_dim: int, dropout: float = 0.1):
        super().__init__()
        
        self.attention = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=heads,
            dropout=dropout,
            batch_first=True
        )
        
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Multi-head attention with residual connection
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # MLP with residual connection
        mlp_out = self.mlp(x)
        x = self.norm2(x + mlp_out)
        
        return x


class SmokeyNetLike(nn.Module):
    """
    Complete SmokeyNet-like architecture following sacred specifications.
    
    Pipeline:
    1. Tile encoding: ResNet-34 per tile
    2. Temporal modeling: LSTM across frames
    3. Spatial reasoning: ViT across tiles
    4. Output heads: Global + auxiliary tile heads
    """
    
    def __init__(
        self,
        num_tiles: int = 45,
        temporal_window: int = 3,
        tile_size: int = 224,
        vit_dim: int = 768,
        vit_depth: int = 6,
        vit_heads: int = 12,
        use_tile_heads: bool = True
    ):
        super().__init__()
        
        self.num_tiles = num_tiles
        self.temporal_window = temporal_window
        self.tile_size = tile_size
        self.use_tile_heads = use_tile_heads
        
        # Sacred components
        self.tile_encoder = TileEncoder()  # ResNet-34
        self.temporal_aggregator = TemporalAggregator()  # Bidirectional LSTM
        
        # Project LSTM output to ViT dimension
        self.temporal_to_vit = nn.Linear(self.temporal_aggregator.output_dim, vit_dim)
        
        self.spatial_aggregator = ViTEncoder(
            dim=vit_dim,
            depth=vit_depth,
            heads=vit_heads
        )  # ViT-S
        
        # Output heads
        self.global_head = nn.Sequential(
            nn.LayerNorm(vit_dim),
            nn.Dropout(0.3),
            nn.Linear(vit_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # Binary classification
        )
        
        # Auxiliary tile heads (optional, for regularization)
        if self.use_tile_heads:
            self.tile_heads = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(vit_dim, 256),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(256, 64),
                    nn.ReLU(),
                    nn.Linear(64, 1)
                )
                for _ in range(num_tiles)
            ])
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Forward pass following sacred SmokeyNet methodology.
        
        Args:
            x: Input tensor [batch_size, temporal_window, channels, height, width]
            Expected: [B, 3, 3, H, W] where H=W after tiling to 224x224
            
        Returns:
            Dictionary with:
            - 'global_logits': [B, 1] - Global smoke probability
            - 'tile_logits': [B, num_tiles] - Per-tile probabilities (if enabled)
        """
        batch_size, L, channels, height, width = x.shape
        
        # Step 1: Tile each frame (sacred preprocessing)
        # This would normally be done in preprocessing, but we'll assume
        # input is already properly formatted for tiles
        
        # For now, we'll work with the assumption that input represents
        # already-tiled sequences. In real implementation, tiling would happen here.
        
        # Step 2: Encode each tile across all frames
        # Reshape for processing: [B*L*num_tiles, C, tile_H, tile_W]
        # For simplicity, we'll work with the full image and simulate tiling
        
        tile_features = []
        
        # Process each frame in the temporal sequence
        for t in range(L):
            frame = x[:, t]  # [B, C, H, W]
            
            # Simulate tiling (in real implementation, this would be proper tiling)
            # For now, we'll just replicate the frame for each tile position
            frame_tiles = []
            for tile_idx in range(self.num_tiles):
                # In real implementation, this would extract actual tile regions
                # For now, we'll use the full frame resized to tile_size
                tile = F.interpolate(frame, size=(self.tile_size, self.tile_size), mode='bilinear')
                tile_feat = self.tile_encoder(tile)  # [B, 512]
                frame_tiles.append(tile_feat)
            
            frame_tile_features = torch.stack(frame_tiles, dim=1)  # [B, num_tiles, 512]
            tile_features.append(frame_tile_features)
        
        # Stack temporal features: [B, L, num_tiles, 512]
        temporal_tile_features = torch.stack(tile_features, dim=1)
        
        # Step 3: Apply temporal aggregation per tile
        # Reshape for LSTM: [B*num_tiles, L, 512]
        B, L, num_tiles, feat_dim = temporal_tile_features.shape
        temporal_input = temporal_tile_features.transpose(1, 2).reshape(B * num_tiles, L, feat_dim)
        
        temporal_output = self.temporal_aggregator(temporal_input)  # [B*num_tiles, L, 1024]
        
        # Take the last timestep output for each tile
        temporal_features = temporal_output[:, -1, :]  # [B*num_tiles, 1024]
        temporal_features = temporal_features.view(B, num_tiles, -1)  # [B, num_tiles, 1024]
        
        # Step 4: Project to ViT dimension
        vit_input = self.temporal_to_vit(temporal_features)  # [B, num_tiles, vit_dim]
        
        # Step 5: Apply spatial aggregation (ViT)
        global_repr, tile_reprs = self.spatial_aggregator(vit_input)
        
        # Step 6: Generate predictions
        global_logits = self.global_head(global_repr)  # [B, 1]
        
        outputs = {'global_logits': global_logits}
        
        if self.use_tile_heads:
            tile_logits = []
            for i, head in enumerate(self.tile_heads):
                tile_logit = head(tile_reprs[:, i])  # [B, 1]
                tile_logits.append(tile_logit)
            
            tile_logits = torch.cat(tile_logits, dim=1)  # [B, num_tiles]
            outputs['tile_logits'] = tile_logits
        
        return outputs
    
    def compute_loss(
        self, 
        outputs: Dict[str, torch.Tensor], 
        global_labels: torch.Tensor,
        tile_labels: Optional[torch.Tensor] = None,
        global_weight: float = 1.0,
        tile_weight: float = 0.3
    ) -> Dict[str, torch.Tensor]:
        """
        Compute loss following sacred specifications.
        Sacred formula: L = Î»_global * BCE + Î»_tiles * BCE_tiles (1.0 y 0.3)
        """
        losses = {}
        
        # Global loss (sacred weight: 1.0)
        global_logits = outputs['global_logits'].squeeze(-1)  # [B]
        global_loss = F.binary_cross_entropy_with_logits(
            global_logits, 
            global_labels.float(),
            pos_weight=torch.tensor(5.0)  # Sacred specification: weight 5 for positives
        )
        losses['global_loss'] = global_loss
        
        total_loss = global_weight * global_loss
        
        # Tile losses (sacred weight: 0.3)
        if self.use_tile_heads and 'tile_logits' in outputs and tile_labels is not None:
            tile_logits = outputs['tile_logits']  # [B, num_tiles]
            tile_loss = F.binary_cross_entropy_with_logits(
                tile_logits, 
                tile_labels.float(),
                pos_weight=torch.tensor(40.0)  # Sacred specification: weight 40 for tile positives
            )
            losses['tile_loss'] = tile_loss
            total_loss += tile_weight * tile_loss
        
        losses['total_loss'] = total_loss
        return losses


def create_smokeynet_like(config: Optional[Dict[str, Any]] = None) -> SmokeyNetLike:
    """
    Factory function to create SmokeyNet-like model with sacred defaults.
    """
    default_config = {
        'num_tiles': 45,      # Sacred specification
        'temporal_window': 3,  # Sacred specification (L=3)
        'tile_size': 224,     # Sacred specification
        'vit_dim': 768,       # Sacred specification
        'vit_depth': 6,       # Sacred specification (6-8 blocks)
        'vit_heads': 12,      # Sacred specification
        'use_tile_heads': True
    }
    
    if config:
        default_config.update(config)
    
    return SmokeyNetLike(**default_config)


if __name__ == "__main__":
    # Test the sacred architecture
    model = create_smokeynet_like()
    
    # Test input: [batch_size=2, temporal_window=3, channels=3, height=224, width=224]
    test_input = torch.randn(2, 3, 3, 224, 224)
    
    with torch.no_grad():
        outputs = model(test_input)
        print("Sacred SmokeyNet-like Architecture Test:")
        print(f"Global logits shape: {outputs['global_logits'].shape}")
        if 'tile_logits' in outputs:
            print(f"Tile logits shape: {outputs['tile_logits'].shape}")
        
        # Test loss computation
        global_labels = torch.tensor([1.0, 0.0])  # [smoke, no-smoke]
        losses = model.compute_loss(outputs, global_labels)
        print(f"Total loss: {losses['total_loss'].item():.4f}")
        print("Sacred architecture test completed successfully! ðŸ”¥")