# Sacred SmokeyNet Training Configuration - Real L=3 Sequences + H200 Optimized
# Using authentic FIgLib wildfire dataset with 37,506 L=3 sequences

# Sacred model architecture (MANTENER EXACTO)
model:
  num_tiles: 45          # âœ… Sacred specification
  temporal_window: 3     # âœ… Sacred specification (L=3)
  tile_size: 224         # âœ… Sacred specification
  vit_dim: 768          # âœ… Sacred specification  
  vit_depth: 6          # âœ… Sacred specification (6-8 blocks)
  vit_heads: 12         # âœ… Sacred specification
  use_tile_heads: true  # âœ… Sacred auxiliary heads

# Sacred training parameters (MANTENER EXACTO)
training:
  learning_rate: 2e-4     # âœ… Sacred specification
  weight_decay: 0.05      # âœ… Sacred specification
  max_epochs: 70          # âœ… Sacred range 60-80
  global_loss_weight: 1.0 # âœ… Sacred specification
  tile_loss_weight: 0.3   # âœ… Sacred specification
  warmup_epochs: 5        # âœ… Sacred specification
  gradient_clip_val: 1.0  # âœ… Sacred specification
  precision: "bf16-mixed" # âœ… Sacred: BF16/FP16 (H200 native support)
  
  # ðŸ”§ H200 OPTIMIZATION (hardware adaptation)
  batch_size: 8           # Conservative for real sequences (vs cache 10)
  accumulate_grad_batches: 8  # BS_eff=64 (sacred target)

# ðŸ”¥ REAL L=3 SEQUENCES Dataset Configuration
data:
  # Real sequence paths (61GB in /dev/shm memory)
  data_root: "data/figlib_seq_real"
  use_real_sequences: true   # Force real sequence usage
  
  # ðŸ”§ H200 OPTIMIZATION (192 cores + 258GB RAM + freed /dev/shm space)
  batch_size: 8           # Match training batch_size
  num_workers: 8          # Optimized for available memory
  pin_memory: true        # âœ… Sacred + abundant RAM
  persistent_workers: true # âœ… Keep workers in 258GB RAM
  prefetch_factor: 2      # Conservative for real loading
  
  # âœ… Sacred specifications (unchanged)
  temporal_window: 3      # Sacred L=3
  tile_size: 224         # Sacred specification
  num_tiles: 45          # Sacred specification

# âœ… Sacred objectives (MANTENER EXACTO)
objectives:
  target_recall: 0.80     # âœ… Sacred: Recall â‰¥ 0.80
  target_f1: 0.826        # âœ… Sacred: F1 â‰ˆ 82.6%
  target_ttd: 4.0         # âœ… Sacred: TTD â‰¤ 4 min

# Logging and checkpoints (mantener sacred)
logging:
  project_name: "sai-net-verificador-real-sequences"
  experiment_name: "smokeynet-like-real-l3-sequences"
  log_every_n_steps: 50   # Less frequent with large dataset

checkpoints:
  monitor: "val/f1"       # âœ… Sacred priority metric
  mode: "max"
  save_top_k: 3
  save_last: true
  filename: "smokeynet-real-l3-epoch{epoch:02d}-f1{val/f1:.4f}"
  dirpath: "outputs/smokeynet/checkpoints"

# Early stopping based on sacred objectives  
early_stopping:
  monitor: "val/f1"       # âœ… Sacred metric
  patience: 12            # Shorter for large dataset
  min_delta: 0.005        # More sensitive for real data
  mode: "max"

# ðŸ”§ H200 Hardware Configuration
trainer:
  devices: 1              # Hardware real: 1Ã—H200
  strategy: "auto"        # No DDP needed
  accelerator: "gpu"      # âœ… Sacred
  sync_batchnorm: false   # Single GPU
  
  # H200 Performance optimizations
  compile: false          # Disable for stability initially
  enable_checkpointing: false  # Sufficient VRAM

# Validation and testing (mantener sacred)
validation:
  check_val_every_n_epoch: 1
  val_check_interval: 1.0

# âœ… Sacred reproducibility
seed: 42

# Model export (sacred pipeline)
export:
  formats: ["onnx", "torchscript"]  # âœ… Sacred: ONNX/TensorRT pipeline
  optimize_for_inference: true
  export_dir: "outputs/smokeynet/exported"

# Performance monitoring
monitoring:
  log_gpu_memory: true
  log_system_stats: true
  profile_dataloader: false  # Enable for debugging only

# System resource limits (safety for real sequences)
limits:
  max_vram_gb: 130        # Conservative limit for H200 (143GB total)
  worker_memory_gb: 6     # Higher per worker for real images